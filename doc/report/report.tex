\documentclass[a4paper,11pt]{article}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
% \usepackage{amsmath}
% \usepackage{amssymb,amsfonts,textcomp}
% \usepackage{color}
% \usepackage{array}
% \usepackage{supertabular}
% \usepackage{hhline}
\usepackage{hyperref}
\usepackage{cite}
% \usepackage{etoolbox}
\usepackage{acro}
\usepackage{graphicx}
\usepackage[nodayofweek]{datetime}

\include{helpers/acronyms}
\DeclareGraphicsRule{*}{mps}{*}{}
\newcommand{\mparagraph}[1]{\paragraph{#1}\mbox{}\\}

% Commands
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines

\begin{document}

	\begin{titlepage}
		\begin{centering}
		 
		%	HEADING SECTIONS
		\textbf{\textit{\large{B.Tech Project Report}}}\\[0.5cm]
		
		\textsc{\textbf{\LARGE{Decision making using deep reinforcement learning}}}\\[1.5cm]

		\large{Submitted in partial fulfilment for the award of the Degree of Bachelor of Technology in Computer Science and Engineering}\\[1.5cm]

		\large{Submitted by}\\[0.5cm]

		\textbf{Jayadeep K M     (Roll No 13400030)}\\
		\textbf{Kevin Joseph     (Roll No 13400032)}\\
		\textbf{Mohammed Nisham K     (Roll No 13400038)}\\[1.5cm]
		
		{Under the guidance of}\\[0.25cm]
		\large{Mr. Vipin Vasu A V}\\[0.5cm]

		\includegraphics[width=5cm]{images/logo.jpg} 

		Department of Computer Science and Engineering\\
		\textsc{College of Engineering, Trivandrum}\\
		\textsc{Kerala}\\
		\textsc{May 2017}\\
		\vfill % Fill the rest of the page with whitespace
		\end{centering}
	\end{titlepage}

	\begin{titlepage}
		\begin{centering}
			\textbf{\textit{\LARGE\textsc{{certificate}}}}\\[0.5cm]
			\includegraphics[width=5cm]{images/logo.jpg}\\

		\end{centering}

		\large{This is to certify that the thesis entitled ``Decision making using deep reinforcement learning'' is a bonafide record of the major project done by \textbf{Jayadeep K M} (Roll No 13400030), \textbf{Kevin Joseph} (Roll No 13400032) and \textbf{Mohammed Nisham K} (Roll No 13400038) under my supervision and guidance, in partial fulfilment for the award of the Degree of Bachelor of Technology in Computer Science and Engineering from the University of Kerala for the year 2017.}\\[1.5cm]

		\begin{minipage}{0.4\textwidth}
		\begin{flushleft}
		\end{flushleft}
		\end{minipage}
		~
		\begin{minipage}{0.6\textwidth}
		\begin{centering} \large
		\large{Mr. Vipin Vasu A V}\\
		\small{(Guide)}\\
		\small{\textit{\textbf{Asst. Professor}}}\\
		\small{\textit{\textbf{Dept. of Computer Science and Engineering}}}\\[1.5cm]

		\large{Mrs. Liji}\\
		\small{\textit{\textbf{Professor and Head}}}\\
		\small{\textit{\textbf{Dept. of Computer Science and Engineering}}}\\
		\end{centering}
		\end{minipage}\\[1.0cm]

		\begin{flushleft}
		Place: Trivandrum\\
		Date:  11-05-2017\\
		\end{flushleft}
		\vfill % Fill the rest of the page with whitespace
	\end{titlepage}

	% TODO Ack before this
	\pagenumbering{roman}
	
	\begin{abstract}
		Creating a general purpose AI has been an area of research since the beginning of computers and programming. Reinforcement learning is a major step towards a general purpose AI. \\

		This project is aimed at creating a program that can learn to make decisions in an environment that is defined by a high-dimensional input, and has sparce and time delayed rewards for these actions. Such programs can be useful in problems where decsion must be made based on high dimensional sensory input such as camera feed. This project uses Q-learning algorithm to assign a quality value to each action in a state of the environment. \\

		Atari games are used to demonstrate this approach, by training the program to play breakout game for upto 50 epochs and observing performance improvement. The trained neural network was saved and tested at the end of every epoch. The performance parameters like average q-value, average reward, games per epoch were also saved. The performace parameters showed a clear rise in performace for breakout (50 epochs) and space invaders (8 epochs). \\

		The project has applications in the field of IOT, security, gaming, stock market analysis and traffic control systems. Any system that can be modelled as an environment with actions and rewards can be trained using this algorithm.

	\end{abstract}
	
	\newpage
	\tableofcontents
	\newpage
	\listoffigures
	\newpage
	\printacronyms[include-classes=abbrev,name=Abbreviations]
	\newpage

	\pagenumbering{arabic}
	\section{Introduction}
		\subsection{Motivation and Overview}
			Learning to control agents directly from high-dimensional sensory inputs like vision and speech is one of the long-standing challenges of Artificial Intelligence and Machine Learning. Most successful \ac{ai} applications have relied on hand-crafted features combined with linear value functions or policy representations. Clearly, the performance of such systems heavily relies on the quality of the feature representation.

			Recent advances in deep learning have made it possible to extract high-level features from raw sensory data, leading to breakthroughs in computer vision and speech recognition. These methods utilise a range of neural network architectures, including convolutional networks, multilayer perceptrons, restricted Boltzmann machines and recurrent neural networks, and have exploited both supervised and unsupervised learning.
	
			However the main advantage of Reinforcement Learning is that it does't need huge amounts of hand-labelled data and does not depend too much on the feature representation. It learns from a reward signal that maybe delayed, noisy and sparse. The delay between actions and the rewards, which maybe thousands of timesteps seems like a particularly hard problem in \ac{rl} when compared to direct association between action and reward in supervised learning. Another issue is that most problems consider all the data samples independent of each other, but in this case we need to consider the fact that the reward at the end of a session is not just the result of the last action but the result of the sequence of actions from the start of the session.

			This project is aimed at using a Convolution Neural Network along with the Q-learning algorithm to solve the above problems and make decisions based on video input from the Atari Learning Environment. Our goal is to create a single neural network agent that is able to successfully learn to play atleast 2 games with no change in agent algorithm. The network was not provided with any game-specific information or hand-designed visual features, and was not privy to then internal state of the emulator, it learned from nothing but the video input, the reward and terminal signals, and the set of possible actions just as a human player would.

		\subsection{Background and Literature Survey}
			\paragraph{Deep Mind and Reinforcement Learning}

				The presented method and algorithm was first presented in a paper by a company Deep Mind which was later aquired by Google. The paper presented the algorithm as a general purpose RL algorithm that could play seven of the atari 2600 games, it showed human level play in four and super human play in three of them.
				Consider the game Breakout. In this game you control a paddle at the bottom of the screen and have to bounce the ball back to clear all the bricks in the upper half of the screen. Each time you hit a brick, it disappears and your score increases you get a reward.

				\begin{figure}[!ht]
					\begin{centering}
						\includegraphics[width=10cm]{../Design/images/breakout.png}\\
						\caption{Breakout screens.}
					\end{centering}
				\end{figure}
				Suppose you want to teach a neural network to play this game. Input to your network would be screen images, and output would be one of four actions left, right, do nothing or fire to launch the ball. It would make sense to treat it as a classification problem for each game screen you have to decide, Which action to take. Sounds straightforward, Sure, but then you need training examples, and a lots of them. Of course you could go and record game sessions using expert players, but thats not really how we learn. We dont need somebody to tell us a million times which move to choose at each screen. We just need occasional feedback that we did the right thing and can then figure out everything else ourselves.

				This is the task reinforcement learning tries to solve. Reinforcement learning lies somewhere in between supervised and unsupervised learning. Whereas in supervised learning one has a target label for each training example and in unsupervised learning one has no labels at all, in reinforcement learning one has sparse and time-delayed labels, the rewards. Based only on those rewards the agent has to learn to behave in the environment.

				While the idea is quite intuitive, in practice there are numerous challenges. For example when you hit a brick and score a reward in the Breakout game, it often has nothing to do with the actions (paddle movements) you did just before getting the reward. All the hard work was already done, when you positioned the paddle correctly and bounced the ball back. This is called the credit assignment problem i.e., which of the preceding actions was responsible for getting the reward and to what extent.

				Once you have figured out a strategy to collect a certain number of rewards, should you stick with it or experiment with something that could result in even bigger rewards? In the above Breakout game a simple strategy is to move to the left edge and wait there. When launched, the ball tends to fly left more often than right and you will easily score about 10 points before you die. Will you be satisfied with this or do you want more? This is called the explore-exploit dilemma should you exploit the known working strategy or explore other, possibly better strategies.

				Reinforcement learning is an important model of how we (and all animals in general) learn. Praise from our parents, grades in school, salary at work these are all examples of rewards. Credit assignment problems and exploration exploitation dilemmas come up every day both in business and in relationships. Thats why it is important to study this problem, and games form a wonderful sandbox for trying out new approaches.

	\section{Materials and Methodology}
		\subsection{Algorithms}
			\subsection{Markov Decision Process}
				Suppose you are an agent, situated in an environment (e.g. Breakout game). The environment is in a certain state (e.g. location of the paddle, location and direction of the ball, existence of every brick and so on). The agent can perform certain actions in the environment (e.g. move the paddle to the left or to the right). These actions sometimes result in a reward (e.g. increase in score). Actions transform the environment and lead to a new state, where the agent can perform another action, and so on. The rules for how you choose those actions are called policy. The environment in general is stochastic, which means the next state may be somewhat random (e.g. when you lose a ball and launch a new one, it goes towards a random direction).
				\begin{figure}[!ht]
					\begin{centering}
						\includegraphics[width=10cm]{../Design/images/mdp.png}\\
						\caption{Breakout screens.}
					\end{centering}
				\end{figure}
				The set of states and actions, together with rules for transitioning from one state to another, make up a Markov decision process. One episode of this process (e.g. one game) forms a finite sequence of states, actions and rewards:

				\begin{centering}
				\includegraphics[width=10cm]{../Design/images/mdp2.png}\\
				\end{centering}

				Here s$_{i}$ represents the state, ai is the action and ri+1 is the reward after performing the action. The episode ends with terminal state sn (e.g. game over screen). A Markov decision process relies on the Markov assumption, that the probability of the next state si+1 depends only on current state si and action ai, but not on preceding states or actions.
			\subsubsection{Q-Learning}
			\subsubsection{Deep Q-Learning}
				\paragraph{Neural Networks}

		\subsection{Program Development}
			\subsubsection{System Description}
			\subsubsection{Class Diagram}
				\begin{figure}[!h]
					\begin{centering}
						\includegraphics[width=15cm]{../uml/uml.1}
						\caption{UML Class Diagram.}
					\end{centering}
				\end{figure}

		\subsection{Code Overview}
			The code for the project is open on github. https://github.com/devilsangel/deepQLearning

			\subsubsection{Environment}
				The environment is an interface that is used to control the environment of the game. It gives the actions that can be peformed at any given time, gets the current state of the system, act a particular action on the system and return the reward if there is one. It also alerts the agent when a terminal state is reached.

				The ALEEnvironment is the class that implements the environment interface. The structure was made so that different environments could be used interchangably with minimal change to the code. Other environments like gym environment can be used to implement the interface as well.

			\subsubsection{Agent}
				Agent class is the crux of the system. Which has variables of all other class types. In train mode, the system trains based on the qvalues provided by the deepQNetwork occasionally correcting it when rewards approach. After every step of the game, a callback to statisctics on step function is called. During training the loss of a life or the end of the game is treated as terminal states. Exploration rate controls whether exploration or exploitation is chosen. 

				In test mode, a given no of games are played, here the statebuffer is used to disallow contamination of trained data. Here only the end of the game is taken as a terminal state.

				In random mode, random actions are given to the system and the output observed.

			\subsubsection{Plotter}
				The plotter class uses python libraries to plot the data saved about each epoch as a graph. Visualising the important data metrics like average score and qvalue of each epoch, which can be used to evaluate the performance of the training.

				It also serves the function of plotting the maximum qvalue of the current screen live during play.
			
			\subsubsection{Statistics}
				The statistics class embeds itself as a callback in the agent class. The agent calls the on step method after every step of the game. If the game reaches a terminal state, the values like average score, no of games etc are updated in the statistics class.

				When the epoch ends, the write method is called which appends the calculated data to the csv file.

			\subsubsection{DeepQNetwork}
				An abstraction layer over Neon, a deep learning library from Nervana Systems. It Models the Network, finds the Q values associated with a state of the system, and does updation of weights based on reward and a random minibatch. The reward is used to refine the discounted future reward of the minibatch.

			\subsubsection{Replay Memory}
				For training the states, actions, screens and rewards need to be saved. These can be added to the replay memory using the add method. When a reward is obtained, a minibuffer is selected with the current state at the index position. This minibuffer is used for training the system.

			\subsubsection{State Buffer}
				During testing and play, the replay memory is not necessary, so a stripped down version of it, storing only the states of the system is used. This has the added advantage of ensuring that the data from testing does not pollute the training data.

	\section{Results and Discussions}
		The results of the training sessions were saved in a csv file in increments of 1 epoch or 250000 steps. Important parameters of the epoch like steps, number of games, average reward, mean cost, average q value etc are saved for three phases, training, test and random.


		\subsection{Description of observed strategies}
			\mparagraph{Breakout}
				Before training the system plays randomly, \emph{ie} chooses an action at random. 

				After one epoch the system adopts a simple strategy, to move to the left of the screen and stay there, an approach that assures a minimum score of two before defeat.

				After ten epochs the system plays like a human player, moving the paddle in response to the ball and dropping it occasionally.

				At fifty epochs, the play is significantly better occasionally resorting to the top strategy of tunnelling to the top of the screen from the edge.

			\mparagraph{Space Invaders}
				Space Invaders was trained for 8 epochs to prove that the algorithm can be used to train and play multiple games without any alterations proving the general purposse nature of it. At 8 epochs the game is played as a human would hitting the targets accurately enough and a consistent score in the range of 400 to 800 is obtained by the system.

		\subsection{Result Visualization}
			The results stored in csv files were plotted as a graph for visualization using matplotlib python module. Training of atari breakout was done for 50 epochs (65 hours). It  adopts visibly better strategies on later epochs. The training of space invaders was done for 8 epochs (10 hours), gameplay after 8 epochs was akin to a human player without any alteration to the algorithm, thus proving the general purpose nature of the algorithm.

			\begin{figure}[!h]
				\begin{centering}
					\includegraphics[width=15cm]{images/breakout.png}
					\caption{Breakout results for 50 epochs.}
				\end{centering}
			\end{figure}			
			
			\begin{figure}[!h]
				\begin{centering}
					\includegraphics[width=15cm]{images/space_invaders.png}
					\caption{Space Invaders results for 8 epochs.}
				\end{centering}
			\end{figure}			

	\section{Further Work}
		\subsection{Gamification of Problem}
			Solving real world problems can be achieved using Deep Reinforcement Learning. The following points are to be considered to convert a real world problem into a deep learning optimisation problem:
			\begin{itemize}
				\setlength\itemsep{0em}
				\item The problem is to be modelled as a game.
				\item The game should include an environment which has a finite set of actions.
				\item High dimensional input to the environment should be available as input and savable as a state.
				\item State of the system should change according to the actions performed on it.
				\item The environment should occasionally give off rewards in response to its current state or action performed.
			\end{itemize}

			If all of the criterions are met and a gamified version of the real world problem is made, then it can be quite easily given to a deep reinforcement learning system for optimisation and then later used in real life after training.

		\subsection{Traffic Light Control}
				A sample real world problem that could be solved using reinforced learning.
			\subsubsection{Problem}
				There is an intersection of two roads, and four traffic lights control the traffic in it, each controlling one road. The control of the traffic light should be given to an \ac{ai} for optimum traffic flow through the intersection. To design a system that can be used to train this \ac{ai}. 

			\subsubsection{Minimal Input}
				Traffic camera images at four different directions are taken as input to the system, mulitple images are taken to incorporate movement of the vehicles. Even though the system could work with the raw images, it would be impossible to train the system in the real world. So a simulation is required to train it. But in a simulation output images cannot be generated as complex as real video stream. 

				The solution is to simplify the input to the system, and to simulate the output in this simplified version. The approach taken is to convert the vehicles into white rectangles in a black canvas, and the intersection as a line. Thus white rectangles passing over the line will be taken for rewards and any intersection of rectangles will be interpreted as a crash. The rectangles are easier to simulate given the actions to be performed.

				To make the simulation closer to reality, delay between the signal turning green and the first car moving can be given. Contraction and elongation of traffic at stop and start can also be simulated.

				\begin{figure}[!h]
					\begin{centering}
						\includegraphics[width=10cm]{images/traffic1.png}
						\caption{Empty Intersection.}
					\end{centering}
				\end{figure}

				
				\begin{figure}[!h]
					\begin{centering}
						\includegraphics[width=10cm]{images/traffic2.png}
						\caption{Intersection with Vehicles.}
					\end{centering}
				\end{figure}

				
				\begin{figure}[!h]
					\begin{centering}
						\includegraphics[width=10cm]{images/traffic3.png}
						\caption{Generation of Simplified Input.}
					\end{centering}
				\end{figure}

				
				\begin{figure}[!h]
					\begin{centering}
						\includegraphics[width=10cm]{images/traffic4.png}
						\caption{Simplified Input.}
					\end{centering}
				\end{figure}



			\subsubsection{Action set}
				At any instant, any of the traffic lights can be any of the following:
				\begin{itemize}
					\setlength\itemsep{0em}
					\item Red
					\item Orange
					\item Green in the Right, Left or Straight direction or any combination of it.
				\end{itemize}

				Orange can be eliminated by making it mandatory for transitions between Green and Red. Green has seven states (not to be confused with state of the system found from the input) it can be in for all the combinations. Since only Green or Red can be active at a given time, the total no of states for a single traffic light is 8.

				The intersection consists of 4 traffic lights, operating independantly (No constraints to its operation) making the total no of actions the system can take to $8^4$ \emph{ie} 4096 actions.

				Thus at every state of the system, it can decide between 4096 actions to take.
			\subsubsection{Reward}
				The no of vehicles that pass through the intersection in a time interval or the throughput of the intersection can be taken as a rudimentary reward function.

				To avoid any traffic signals that might incur collisions, any collisions detected are penalised with a negative reward. This still presents the problem of starvation \emph{ie.} a low amount of vehicles could wait for green indefinitely. It can be solved by aging the throughput negatively, which makes the throughput reward smaller and finally negative the longer it waits.
			\subsubsection{Scalability}
				The system proposed could be implemented on a larger scale, like a town or city. But in the current form of neural networks, adding a single traffic light to an existing system makes retraining the entire system a requirement. As the number of intersections increase, the number of actions rise exponentially. Thus making their efficient implementation a problem.

				The bottleneck is the non scalability of the current architecture, which could be tentatively solved if parallel neural networks are introduced. Parallel neural networks are still in its infancy and outside the scope of this project.
	\section{Conclusion}



	\newpage
	\nocite{*}
	\bibliography{helpers/bibliography}
	\bibliographystyle{ieeetr}

\end{document}